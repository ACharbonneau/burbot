---
title: "burbot_assembly"
author: "Amanda"
date: "3/27/2018"
output: md_document
---


# List of references
http://oyster-river-protocol.readthedocs.io/en/latest/

# Setting up an AWS instance for the first time: 

Followed the "Launch a Linux Virtual Machine" tutorial on AWS to learn how to create a new Amazon instance (https://aws.amazon.com/getting-started/tutorials/launch-a-virtual-machine/) 

Accoring to the Oyster River Protocol (https://github.com/macmanes-lab/orp_website/blob/master/aws_setup.md) we want to run everything on a standard Ubuntu 16.04 machine, so I selected this type of instance in AWS. Set up the instance with the default settings. Downloaded my key (mandykey), which is required to access the instance from the Terminal. 

You must move the key from the Downloads file to your ssh folder and make the key invisible: 

```{bash}
mv ~/Downloads/mandykey.pem ~/.ssh/mandykey.pem
chmod 400 ~/.ssh/mandykey.pem
```


# Connecting to AWS: 

To connect to the instance in the Terminal, you have to go to the instance and retrieve the Public DNS (ec2... blah blah). This will change every time you log on, so you'll have to check every time. Now you can log on through the Terminal:

```{bash}

ssh -i ~/.ssh/mandykey.pem -l ubuntu ec2-35-166-55-218.us-west-2.compute.amazonaws.com

```

When you are done running whatever you are doing, go back to your AWS webpage, right click on "Running" under the "Instance State" tab, scroll to "Instance State" and click "Stop". Do not click "Terminate", because this will kill the instance. You need to stop your instance when you're done so that you don't get charged for hours that you're not actually using! 

#Create Alarms: 
What happens if you forget to stop your instance? What happens if a major job fails when you're using a higher powered (=expensive) computer and you don't realize? You get charged for hours that you're not using! This is why we want to create an alarm to stop the instance when it's not in use. Here are the settings I chose:
Under the "Monitoring" tab, select "Create Alarm": 
- Send a notification to "InstanceStopped" with these recipients: ajfrazier@ucdavis.edu
- Take the action "Stop this instance"
- Whenever "Average" of "CPU utilization" is <= 1% for at least 1 consecutive period(s) of 1 hour. 


#Install Programs: 

I attempted to follow the installation instructions of the Oyster River protocol: (https://github.com/macmanes-lab/orp_website/blob/master/aws_setup.md)
But, his "streamlined" download for the entire ORP didn't work. So, I ended up installing everything individually. 

Update software and install linuxy things from apt-get: 
```{bash}

sudo apt-get update && sudo apt-get -y upgrade && sudo apt-get -y install ruby build-essential mcl python python-pip default-jre

```


Install linux brew: 
```{bash}

sudo mkdir /home/linuxbrew
sudo chown $USER:$USER /home/linuxbrew
git clone https://github.com/Linuxbrew/brew.git /home/linuxbrew/.linuxbrew
echo 'export PATH="/home/linuxbrew/.linuxbrew/bin:$PATH"' >> ~/.profile
echo 'export MANPATH="/home/linuxbrew/.linuxbrew/share/man:$MANPATH"' >> ~/.profile
echo 'export INFOPATH="/home/linuxbrew/.linuxbrew/share/info:$INFOPATH"' >> ~/.profile
source ~/.profile
brew tap brewsci/science
brew tap brewsci/bio
brew update
brew install gcc python metis parallel

```

Install python modules:
```{bash}

pip install cvxopt numpy biopython scipy
pip install --upgrade pip

```

Install the Oyster River Protocol:
```{bash}

git clone https://github.com/macmanes-lab/Oyster_River_Protocol.git
cd Oyster_River_Protocol
make -j4

### Make sure to add the items to your profile file, as needed.
### Make sure to ```source``` the profile file after, to make sure everything is loaded. 


```

Set up BUSCO: 
```{bash}

# Download databases
mkdir $HOME/Oyster_River_Protocol/busco_dbs && cd $HOME/Oyster_River_Protocol/busco_dbs

# Eukaryota
wget http://busco.ezlab.org/v2/datasets/eukaryota_odb9.tar.gz
wget http://busco.ezlab.org/v2/datasets/fungi_odb9.tar.gz
wget http://busco.ezlab.org/v2/datasets/metazoa_odb9.tar.gz
wget http://busco.ezlab.org/v2/datasets/nematoda_odb9.tar.gz
wget http://busco.ezlab.org/v2/datasets/arthropoda_odb9.tar.gz
wget http://busco.ezlab.org/v2/datasets/insecta_odb9.tar.gz
wget http://busco.ezlab.org/v2/datasets/vertebrata_odb9.tar.gz
wget http://busco.ezlab.org/v2/datasets/tetrapoda_odb9.tar.gz
wget http://busco.ezlab.org/v2/datasets/aves_odb9.tar.gz
wget http://busco.ezlab.org/v2/datasets/mammalia_odb9.tar.gz

tar -zxf eukaryota_odb9.tar.gz
cd

### Move and edit config file (change everyplace it says `mmacmane` to your user name)

mv $HOME/Oyster_River_Protocol/software/config.ini $HOME/Oyster_River_Protocol/software/busco/config/config.ini

nano $HOME/Oyster_River_Protocol/software/busco/config/config.ini


### add this line under the `[busco] line`

lineage_path = $HOME/Oyster_River_Protocol/busco_dbs/eukaryota_odb9

### obviously, if you're using another database, that name will change.


```

Test the installation: 
```{bash}
cd $HOME/Oyster_River_Protocol/sampledata

../oyster.mk main \
MEM=15 \
CPU=8 \
READ1=test.1.fq.gz \
READ2=test.2.fq.gz \
RUNOUT=test
```

Testing the installation didn't work (error: "*** SALMON is not installed, must fix ***"), so we looked in all the bin's to find where these types of programs were installed. In /home/ubuntu/Oyster_River_Protocol there is a "pathfile" that has specified the path to a bunch of things (e.g. blast, transrate, etc.) but not salmon, so we added the location of salmon to the end of the pathfile, hoping that this would specify where Salmon is installed and then it would run. 

```{bash}
PATH=$PATH:/home/ubuntu/Oyster_River_Protocol/software/orp-transrate/bin 
```


#Notes on installing...
Welp, trying to install everything using the "streamlined" ORP makefile didn't work. It didn't correctly install all the dependnecies etc. We think it's the best move to start a new Amazon Instance and just install everything piecemiel myself. 

#Script to run trinity:
```{bash}

#!/bin/bash

echo "Running trinity..."

~/Oyster_River_Protocol/software/trinityrnaseq/Trinity --seqType fq --max_memory 1G --CPU 1 --output Rcorr_trinity --full_cleanup --left test.1.cor.fq --right test.2.cor.fq --no_normalize_reads --no_bowtie

```

#Install linuxy things from apt-get: 
```{bash}

sudo apt-get update && sudo apt-get -y upgrade && sudo apt-get -y install ruby build-essential mcl python python-pip default-jre

```

#Install linux brew: 
```{bash}

sudo mkdir /home/linuxbrew
sudo chown $USER:$USER /home/linuxbrew
git clone https://github.com/Linuxbrew/brew.git /home/linuxbrew/.linuxbrew
echo 'export PATH="/home/linuxbrew/.linuxbrew/bin:$PATH"' >> ~/.profile
echo 'export MANPATH="/home/linuxbrew/.linuxbrew/share/man:$MANPATH"' >> ~/.profile
echo 'export INFOPATH="/home/linuxbrew/.linuxbrew/share/info:$INFOPATH"' >> ~/.profile
source ~/.profile
brew tap brewsci/science
brew tap brewsci/bio
brew update
brew install gcc python metis parallel

```

# Install python modules:
```{bash}

pip install cvxopt numpy biopython scipy
pip install --upgrade pip

```

# Create a new volume on AWS:
On AWS webpage: 
1. Go to "Volumes" tab
2. Select "Create Volume" to create volume. Make sure the Availability Zone matches the root volume for that instance. 
3. Name the volume you just created.
Note: After you create the volume, it can take up to 6 hours until it's available for use. 
4. Right click on the Volume and select "Attach". Choose the AWS instance that you want to attach it to. Now, when you type "lsblk" in the Terminal, the Volume sould be available. 
In the Terminal:

From Taylor: 
```{bash}
sudo mkfs -t ext4 /dev/xvdf   #DO NOT RERUN THIS AFTER THE FIRST TIME. It will reformat the volume!

sudo mount /dev/xvdf /mnt/    #NOTE: Every time you log onto your instance, run this AND ONLY this line to mount the volume to your instance. 

sudo chown ubuntu -R /mnt/    #This gives you ownership of the new volume. Only do this the first time you set it up. 

cd /mnt/ # to enter the volume you just made. This is where you will store the data.
df -h # to check what you have available to you 

```

My code: 
```{bash}
# 5. Format the volume. DO NOT RUN THIS AFTER THE FIRST TIME as it will reformat the volume and delete any data on it. 
sudo mkfs -t ext4 volumename   
sudo mkfs -t ext4 /dev/xvdg 

# 6. Make a directory where you want to mount your volume: 
sudo mkdir data

# 7. Mount the volume to that directory. This is where it will be in your file-tree: 
sudo mount /dev/xvdg data

# 8. Give yourself ownership of the new volume: 
sudo chown ubuntu -R data 

# 9. Move into the new volume: 
cd data 

# 10. Check what you have available to you:
df -h

```


# Install test data (just a random sample that Erin picked) set on my AWS Instance:
```{bash}
mkdir testburbot

wget http://slimsdata.genomecenter.ucdavis.edu/Data/xzsdtmj958/UnalignedL2toL5/Project_ATMF_L2_H1181P_Frazier/burbot-Ma01-C_S179_L002_R2_001.fastq.gz

wget http://slimsdata.genomecenter.ucdavis.edu/Data/xzsdtmj958/UnalignedL2toL5/Project_ATMF_L2_H1181P_Frazier/burbot-Ma01-C_S179_L002_R2_001.fastq.gz


#Extract first 100000 reads from random burbot sample. This will be the test burbot set. 
for filename in *.fastq.gz; do gunzip -c $filename | head -400000 | gzip > "$filename.extract.fastq.gz"; done

#Ugh, filenames are weird. Correct this:
mv burbot-Ma01-C_S179_L002_R1_001.fastq.gz.extract.fastq.gz burbot-Ma01-C_S179_L002_R1_001.extract.fastq.gz
mv burbot-Ma01-C_S179_L002_R2_001.fastq.gz.extract.fastq.gz burbot-Ma01-C_S179_L002_R2_001.extract.fastq.gz

```

# Install programs (separately)
# Rcorrector
```{bash}
# https://github.com/mourisl/Rcorrector
https://gigascience.biomedcentral.com/articles/10.1186/s13742-015-0089-y

git clone https://github.com/mourisl/rcorrector.git #clone the rcorrector github repo

make #Run this in the rcorrector directory. This script will install rcorrector and install jellyfish2 if you don't already have it. 

perl run_rcorrector.pl -1 Sample/sample_read1.fq -2 Sample/sample_read2.fq #test rcorrector using their test data -- success!

```

# Run rcorrector on my test burbot dataset
```{bash}
cd rcorrector

perl ~/rcorrector/run_rcorrector.pl -1 /home/ubuntu/testburbot/burbot-Ma01-C_S179_L002_R1_001.extract.fastq.gz -2 /home/ubuntu/testburbot/burbot-Ma01-C_S179_L002_R2_001.extract.fastq.gz 

#Success!

#Test the for loop that I have for running rcorrector on all the files:

cd testburbot

for infile in *R1_001.extract.fastq.gz
do
    base=$(basename $infile R1_001.extract.fastq.gz)
    perl ~/rcorrector/run_rcorrector.pl -1 ${base}R1_001.extract.fastq.gz -2 ${base}R2_001.extract.fastq.gz -k 32 
done

#Success!

  

```

#Rcorrector 
##Script
Script "runRcorrector.sh" to run Rcorrector on the concatenated fastq files. 
```{bash}

#!/bin/bash 

for infile in *R1.fastq.gz
do
    base=$(basename $infile R1.fastq.gz)
    echo "Running ${base}"
    perl ~/rcorrector/run_rcorrector.pl -1 ${base}R1.fastq.gz -2 ${base}R2.fastq.gz -k 32 -t 46 -od ../Rcorrected
    echo "Done with ${base}... moving on"
done

#For rcorrector, specify a kmer length of 32 (this is the max kmer length). This makes sense for your data because you have long reads. Default is 23. 
# t=46 is 46 threads to parallelize to decrease running time. 

#Erin's took 20 hours to run. Mine took ~40 hours to run on m5.12xlarge instance

```

Running runRcorrector.sh
```{bash}

#On AWS, increased instance size to t2.2xlarge 

cd data/
mkdir Rcorrected 
cd data/concat

chmod u+x runRcorrector.sh 


nohup ./runRcorrector.sh >runRcorrectorLog.sh &

#Started on Thursday ~2:30 pm. 
#Got stuck.... 

#Took out the -verbose flag, and tried again: 

nohup ./runRcorrector.sh >runRcorrectorLog2.txt &

```


# Install Trinity. Trimmomatic will be installed with Trinity. 
```{bash}

https://github.com/trinityrnaseq/trinityrnaseq/wiki/Installing-Trinity

wget https://github.com/trinityrnaseq/trinityrnaseq/releases/download/Trinity-v2.6.6/Trinityrnaseq-v2.6.6.wExtSampleData.tar.gz

tar -xvf Trinityrnaseq-v2.6.6.wExtSampleData.tar.gz

cd Trinityrnaseq-v2.6.6/

make
make plugins #Install plugins for downstream analyses 
make install #Install Trinity in a central location. This didn't work, but it's not required so moving on. 

#Passed all of the installation tests 

rm Trinityrnaseq-v2.6.6.wExtSampleData.tar.gz 

#Failed the Trinity test data set. "Error, cannot find samtools. Please be sure samtools is installed and included in your PATH setting."


```

# Install Samtools:
```{bash}
https://gist.github.com/adefelicibus/f6fd06df1b4bb104ceeaccdd7325b856

wget https://github.com/samtools/samtools/releases/download/1.3.1/samtools-1.3.1.tar.bz2 -O samtools.tar.bz2
tar -xjvf samtools.tar.bz2
cd samtools-1.3.1
make

rm samtools.tar.bz2
```

# Install conda and Bowtie2:
```{bash}
http://www.xavierdupre.fr/app/pymyinstall/helpsphinx/blog/2015/2015-11-01_anaconda_ssh.html

curl 'https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh' > Miniconda3.sh
bash Miniconda3.sh
cd /home/ubuntu/miniconda3/bin
./conda update conda

conda config --add channels defaults
conda config --add channels conda-forge
conda config --add channels bioconda

conda install bowtie2
conda update bowtie2

```

# Install Salmon: 
```{bash}

conda install salmon
conda update salmon

```

# Add things to my path so they can be run universally! 
```{bash}
#To check what is in your global path: 
echo $PATH

#Apparently there isn't really a difference between .profile and .bashrc, but if a program is in .profile and it's not running universally try putting it in .bashrc and vice versa. 

#The code below will put those programs in the $PATH, but they won't keep them there if you log out (it's not permanent). If you want it to be permanent, do this: 

ls -lah 
nano .profile #.profile is a hidden file 
# Copy and paste "export PATH=$PATH:/{path to program}" in the .profile. This will make it load every time you log on. 
export PATH=$PATH:/home/ubuntu/Trinityrnaseq-v2.6.6
export PATH=$PATH:/home/ubuntu/rcorrector/jellyfish/bin
export PATH=$PATH:/home/ubuntu/samtools-1.3.1
export PATH=$PATH:/home/ubuntu/miniconda3/bin
export PATH=$PATH:/

#If you edit your .profile or .bashrc, you have to "source" it (basically loads it): 

source ~/.profile 
#or 
source ~/.bashrc

```


# Test Trinity:
```{bash}
#On Trinity test set:
cd sample_data/test_Trinity_Assembly/
nohup ./runMe.sh >log.txt &
      #The Trinity test on their data set didn't work. I think it's because I am running it on a tiny computer with one thread, but the bowtie2 step asks for 4 threads. Moving on... 

#On burbot:

cd /home/ubuntu/testburbot

#Made script called "runMe.sh"

#!/bin/bash

cd /home/ubuntu/testburbot

echo "Running trinity..."

/home/ubuntu/Trinityrnaseq-v2.6.6/Trinity --seqType fq --max_memory 1G --CPU 1 --output Rcorr_trinity       --full_cleanup --left burbot-Ma01-C_S179_L002_R1_001.extract.cor.fq.gz --right     burbot-Ma01-C_S179_L002_R2_001.extract.cor.fq.gz --no_normalize_reads

#seqType fq indicates that the data is FASTQ file format
#JM indicates the lower limit of RAM to allocate
# --left indicates which sequence is the forward
# --right indicates which sequence is the reverse
# --CPU indicates how much CPU to allocate. This number is the maximum number of parallel processes, so you can take advantage of parallel processing to speed up the runtime. 
# --no_normalize_reads specifies to NOT normalize (which is the default). Normalization can aide in reducing computing and storage resources, but if we can run the assembly without normalization that would be good. 

chmod u+x runMe.sh 

nohup ./runMe.sh >log.txt & #This will run the Trinity script in the background and print everything to the logfile.

#It worked! Created a Trinity.fasta file called "Rcorr_trinity.Trinity.fasta file"

#Run a basic statiscs check:

cd ~/Trinityrnaseq-v2.6.6/util
./TrinityStats.pl ~/testburbot/Rcorr_trinity.Trinity.fasta

#Output: 

#    ################################
#    ## Counts of transcripts, etc.
#    ################################
#    Total trinity 'genes':	4160
#    Total trinity transcripts:	4586
#    Percent GC: 51.17
#
#    ########################################
#    Stats based on ALL transcript contigs:
#    ########################################
#
#	    Contig N10: 1732
#	    Contig N20: 1152
#	    Contig N30: 726
#	    Contig N40: 532
#	    Contig N50: 424
#
#	    Median contig length: 288
#	    Average contig: 415.42
#	    Total assembled bases: 1905110
#
#
#    #####################################################
#    ## Stats based on ONLY LONGEST ISOFORM per 'GENE':
#    #####################################################
#
#	    Contig N10: 1553
#	    Contig N20: 862
#	    Contig N30: 596
#	    Contig N40: 469
#	    Contig N50: 387
#
#	    Median contig length: 286
#	    Average contig: 391.00
#	    Total assembled bases: 1626542

```

Now I want to see if these results differ with normalization. Let's test it: 

```{bash}

#Made script called "runMeNormal.sh"

#!/bin/bash

cd /home/ubuntu/testburbot

echo "Running trinity..."

/home/ubuntu/Trinityrnaseq-v2.6.6/Trinity --seqType fq --max_memory 14G --CPU 3 --output Rcorr_trinity_norm --full_cleanup --left burbot-Ma01-C_S179_L002_R1_001.extract.cor.fq.gz --right burbot-Ma01-C_S179_L002_R2_001.extract.cor.fq.gz

#Must make script executable: 

chmod u+x runMeNormal.sh 

#Run this in the command line to print output to a logfile instead of the screen and run it on background:

nohup ./runMeNormal.sh >logNormal.txt &


#Failed because ran out of space on device. How do I know how much memory my AWS instance has? What's in the "ephermeral storage" on the AWS instance? What will remain and what will disappear? 

#Note: After this point I tried to increase the volume on AWS, but since I was working on the root volume that caused huge problems. Only install programs on the root volume. Keep data on a separate volume that you can attach and detach. This makes resizing volumes way easier. 

```

#Second time running test Trinity Stats. 
This is after re-doing the entire installation process (above) on a new AWS instance that had a separate volume for my data. 
```{bash}
#Without Normalization: 

cd ~/Trinityrnaseq-v2.6.6/util
./TrinityStats.pl /mnt/testburbot/Rcorr_trinity.Trinity.fasta

#Ouput: 

#     ################################
#     ## Counts of transcripts, etc.
#     ################################
#     Total trinity 'genes':	4167
#     Total trinity transcripts:	4595
#     Percent GC: 51.17
#
#     ########################################
#     Stats based on ALL transcript contigs:
#     ########################################
#
#       Contig N10: 1728
#     	Contig N20: 1138
#     	Contig N30: 740
#     	Contig N40: 537
#     	Contig N50: 425
#
#     	Median contig length: 289
#     	Average contig: 416.36
#       Total assembled bases: 1913190
#
#
#     #####################################################
#     ## Stats based on ONLY LONGEST ISOFORM per 'GENE':
#     #####################################################
#
#     	Contig N10: 1577
#     	Contig N20: 896
#     	Contig N30: 619
#     	Contig N40: 475
#     	Contig N50: 391
#
#     	Median contig length: 286
#     	Average contig: 393.37
#     	Total assembled bases: 1639154


#With normalizatoin: 

cd ~/Trinityrnaseq-v2.6.6/util
./TrinityStats.pl /mnt/testburbot/Rcorr_trinity_norm.Trinity.fasta


#     #Output: 
#     
#     ################################
#     ## Counts of transcripts, etc.
#     ################################
#     Total trinity 'genes':	4135
#     Total trinity transcripts:	4598
#     Percent GC: 51.18
#
#     ########################################
#     Stats based on ALL transcript contigs:
#     ########################################
#
#     	Contig N10: 1914
#     	Contig N20: 1268
#     	Contig N30: 780
#     	Contig N40: 551
#     	Contig N50: 429
#     
#     	Median contig length: 288
#     	Average contig: 422.47
#     	Total assembled bases: 1942500
#
#
#     #####################################################
#     ## Stats based on ONLY LONGEST ISOFORM per 'GENE':
#     #####################################################
#
#     	Contig N10: 1501
#     	Contig N20: 828
#     	Contig N30: 586
#     	Contig N40: 460
#     	Contig N50: 380
#
#     	Median contig length: 284
#     	Average contig: 386.34
#     	Total assembled bases: 1597499


# The results are super similar between the normalized and not-normalized assemblies. This is probably because this data set is just too small to really compare what normalization will do. 

```

Some exploration: 

What's the longest transcript that I got from both the normalized and not-normalized test assemblies? 

```{bash}
cd /mnt/testburbot/

#To cut out the length of each fastq sequence:

grep ">" Rcorr_trinity.Trinity.fasta | cut -f2 -d " " | head   

#To just get the number:
grep ">" Rcorr_trinity.Trinity.fasta | cut -f2 -d " " | cut -f2 -d "=" | head

#To get the longest sequences in order of longest to shortest: 
grep ">" Rcorr_trinity.Trinity.fasta | cut -f2 -d " " | cut -f2 -d "=" | sort -nr | head 

#To get the entire fastq sequence of the the longest file: 

grep -A1 "len=4543" Rcorr_trinity.Trinity.fasta

```

Output from BLAST: 
PREDICTED: Acanthochromis polyacanthus junctional adhesion molecule
Query Cover: 84%
E-value: 2e-82
Ident: 71%

Note that nothing that came up was from the Atlantic Cod genome... 

#Now, on the normalized reads: 

```{bash}
grep ">" Rcorr_trinity_norm.Trinity.fasta | cut -f2 -d " " | cut -f2 -d "=" | sort -nr | head 
grep -A1 "len=5717" Rcorr_trinity_norm.Trinity.fasta

```

Output from BLAST: 
PREDICTED: Seriola lalandi dorsalis collagen alpha-1(I) chain-like
Query cover: 98% 
E-value: 0
Ident: 81%

Again, no cod. If this keeps happening, it indicates that the cod genome is probably not of very good quality. Because of this, I might want to map against different genomes (e.g. amberjack fishes, which have come up mulitple times). 


# Install Busco

```{r}

# To install busco and dependencies: 
conda install busco 

#To download eukaryota dataset: 
wget http://busco.ezlab.org/datasets/eukaryota_odb9.tar.gz

```

# Running RCorrector

##Concatenate files
Script to concatenate the four files from each lane together called "catmyR1.sh". This way I'll only have one R1 file for each fish instead of four. 
```{bash}

#!/bin/bash

mkdir concat
for file_start in burbot-Ma01*R1* burbot-Ma02*R1* burbot-Ma03*R1* burbot-Ma04*R1* burbot-Ma07*R1* burbot-Ma10*R1* burbot-Ma11*R1* burbot-Ma12*R1* burbot-Mb01*R1* burbot-Mb02*R1* burbot-Mb03*R1* burbot-Mb05*R1* burbot-Mb08*R1* burbot-Mb09*R1* burbot-Mb10*R1* burbot-Mb12*R1* burbot-Mc01*R1* burbot-Mc02*R1* burbot-Mc03*R1* burbot-Mc04*R1* burbot-Mc08*R1* burbot-Mc10*R1* burbot-Mc11*R1* burbot-Mc12*R1* burbot-Md01*R1* burbot-Md02*R1* burbot-Md04*R1* burbot-Md05*R1* burbot-Md09*R1* burbot-Md10*R1* burbot-Md11*R1* burbot-Md12*R1* burbot-RV01*R1* burbot-RV02*R1* burbot-RV04*R1* burbot-RV06*R1* burbot-RV08*R1* burbot-RV09*R1* burbot-RV11*R1* burbot-RV12*R1*
do  
  #List the file names we want to concatenate 
  allR1=`ls ./*/Unaligned*/*/${file_start}.fastq.gz`
  #Make the basename of the new concatenated file
  anR1=`ls ./xzsdtmj958/Unaligned*/*/${file_start}.fastq.gz`
  outfile=$(basename $anR1 _L002_R1_001.fastq.gz)
  #Check the new file basename
  echo ${outfile}_R1 
  #Concatenate and write to a new file
  cat $allR1 > ./concat/${outfile}_R1.fastq.gz
done

```

Running catmyR1.sh:
```{bash}

chmod u+x catmyR1.sh

nohup ./catmyR1.sh >catmyR1Log.txt &

```


And again for R2 called "catmyR2.sh"
```{bash}

#!/bin/bash

for file_start in burbot-Ma01*R2* burbot-Ma02*R2* burbot-Ma03*R2* burbot-Ma04*R2* burbot-Ma07*R2* burbot-Ma10*R2* burbot-Ma11*R2* burbot-Ma12*R2* burbot-Mb01*R2* burbot-Mb02*R2* burbot-Mb03*R2* burbot-Mb05*R2* burbot-Mb08*R2* burbot-Mb09*R2* burbot-Mb10*R2* burbot-Mb12*R2* burbot-Mc01*R2* burbot-Mc02*R2* burbot-Mc03*R2* burbot-Mc04*R2* burbot-Mc08*R2* burbot-Mc10*R2* burbot-Mc11*R2* burbot-Mc12*R2* burbot-Md01*R2* burbot-Md02*R2* burbot-Md04*R2* burbot-Md05*R2* burbot-Md09*R2* burbot-Md10*R2* burbot-Md11*R2* burbot-Md12*R2* burbot-RV01*R2* burbot-RV02*R2* burbot-RV04*R2* burbot-RV06*R2* burbot-RV08*R2* burbot-RV09*R2* burbot-RV11*R2* burbot-RV12*R2*
do  
  #List the file names we want to concatenate 
  allR2=`ls ./*/Unaligned*/*/${file_start}.fastq.gz`
  #Make the basename of the new concatenated file
  anR2=`ls ./xzsdtmj958/Unaligned*/*/${file_start}.fastq.gz`
  outfile=$(basename $anR2 _L002_R2_001.fastq.gz)
  #Check the new file basename
  echo ${outfile}_R2 
  #Concatenate and write to a new file
  cat $allR2 > ./concat/${outfile}_R2.fastq.gz
done

```


Running catmyR2.sh:
```{bash}

chmod u+x catmyR2.sh

nohup ./catmyR2.sh >catmyR2Log.txt &

```


# Download sequence data to AWS. 
Download sequence data to AWS using a script called getmydata.sh. Since I have four data directories on SLIMS, I just changed the random string at the end for the four directories and ran the script 4 times. Could do it in parallel, but not sure how...
```{bash}

#!/bin/bash

rsync -avL slimsdata.genomecenter.ucdavis.edu::slims/w4h63my7gq .

```

Run the script on background and record the log:
```{bash}

chmod u+x ./getmydata.sh

nohup ./getmydata.sh >datatransferlog.txt &

```

#Check for data corruption.
##Count number of files in directory.
Count number of files in directory to check if all the files are there: 
```{bash}
ls -l | wc -l 
```

##Check md5
Note that on a mac, the command to check md5 is just md5. In linux, it's md5sum. The output is also slightly different, but it gives you the same md5 hash. If you run md5sum in linux (on AWS) you'll get the output that matches the md5.txt that the genome center gave you. This way you can just compare the two files using diff. If the files match, diff won't print anything. 
```{bash}

diff <(md5sum *.fastq.gz | sort -u) <(cat md5.txt | sort -u)

# All files matched, meaning that data transferred without corruption! 

```

#Run Rcorrector (for realz)

##Script 
```{bash}

#!/bin/bash 



for infile in *R1.fastq.gz
do
    base=$(basename $infile R1.fastq.gz)
    echo "Running ${base}"
    perl ~/rcorrector/run_rcorrector.pl -1 ${base}R1.fastq.gz -2 ${base}R2.fastq.gz -k 32 -t 46 -od ../Rcorrected
    echo "Done with ${base}... moving on"
done

#For rcorrector, specify a kmer length of 32 (this is the max kmer length). This makes sense for your data because you have long reads. Default is 23. 
# t=46 is 46 threads to parallelize to decrease running time. 

#Erin's took 20 hours to run. Mine took ~40 hours to run on m5.12xlarge instance

```

##Running script
```{bash}

```




#Run Trinity (for realz) 

##Script 
```{r Trinity Script, eval=FALSE}

#!/bin/bash

echo $(date)

cd /home/ubuntu/data/2_Rcorrector/Rcorrected2/

R1="burbot-Ma01-C_S179_R1.cor.fq.gz,burbot-Ma02-C_S189_R1.cor.fq.gz,burbot-Ma03-C_S199_R1.cor.fq.gz,burbot-Ma04-C_S209_R1.cor.fq.gz,burbot-Ma07-P_S180_R1.cor.fq.gz,burbot-Ma10-P_S190_R1.cor.fq.gz,burbot-Ma11-P_S200_R1.cor.fq.gz,burbot-Ma12-P_S210_R1.cor.fq.gz,burbot-Mb01-C_S181_R1.cor.fq.gz,burbot-Mb02-C_S191_R1.cor.fq.gz,burbot-Mb03-C_S201_R1.cor.fq.gz,burbot-Mb05-C_S211_R1.cor.fq.gz,burbot-Mb08-P_S182_R1.cor.fq.gz,burbot-Mb09-P_S192_R1.cor.fq.gz,burbot-Mb10-P_S202_R1.cor.fq.gz,burbot-Mb12-P_S212_R1.cor.fq.gz,burbot-Mc01-C_S183_R1.cor.fq.gz,burbot-Mc02-C_S193_R1.cor.fq.gz,burbot-Mc03-C_S203_R1.cor.fq.gz,burbot-Mc04-C_S213_R1.cor.fq.gz,burbot-Mc08-P_S184_R1.cor.fq.gz,burbot-Mc10-P_S194_R1.cor.fq.gz,burbot-Mc11-P_S204_R1.cor.fq.gz,burbot-Mc12-P_S214_R1.cor.fq.gz,burbot-Md01-C_S185_R1.cor.fq.gz,burbot-Md02-C_S195_R1.cor.fq.gz,burbot-Md04-C_S205_R1.cor.fq.gz,burbot-Md05-C_S215_R1.cor.fq.gz,burbot-Md09-P_S186_R1.cor.fq.gz,burbot-Md10-P_S196_R1.cor.fq.gz,burbot-Md11-P_S206_R1.cor.fq.gz,burbot-Md12-P_S216_R1.cor.fq.gz,burbot-RV01-C_S187_R1.cor.fq.gz,burbot-RV02-C_S197_R1.cor.fq.gz,burbot-RV04-C_S207_R1.cor.fq.gz,burbot-RV06-C_S217_R1.cor.fq.gz,burbot-RV08-P_S188_R1.cor.fq.gz,burbot-RV09-P_S198_R1.cor.fq.gz,burbot-RV11-P_S208_R1.cor.fq.gz,burbot-RV12-P_S218_R1.cor.fq.gz"

R2="burbot-Ma01-C_S179_R2.cor.fq.gz,burbot-Ma02-C_S189_R2.cor.fq.gz,burbot-Ma03-C_S199_R2.cor.fq.gz,burbot-Ma04-C_S209_R2.cor.fq.gz,burbot-Ma07-P_S180_R2.cor.fq.gz,burbot-Ma10-P_S190_R2.cor.fq.gz,burbot-Ma11-P_S200_R2.cor.fq.gz,burbot-Ma12-P_S210_R2.cor.fq.gz,burbot-Mb01-C_S181_R2.cor.fq.gz,burbot-Mb02-C_S191_R2.cor.fq.gz,burbot-Mb03-C_S201_R2.cor.fq.gz,burbot-Mb05-C_S211_R2.cor.fq.gz,burbot-Mb08-P_S182_R2.cor.fq.gz,burbot-Mb09-P_S192_R2.cor.fq.gz,burbot-Mb10-P_S202_R2.cor.fq.gz,burbot-Mb12-P_S212_R2.cor.fq.gz,burbot-Mc01-C_S183_R2.cor.fq.gz,burbot-Mc02-C_S193_R2.cor.fq.gz,burbot-Mc03-C_S203_R2.cor.fq.gz,burbot-Mc04-C_S213_R2.cor.fq.gz,burbot-Mc08-P_S184_R2.cor.fq.gz,burbot-Mc10-P_S194_R2.cor.fq.gz,burbot-Mc11-P_S204_R2.cor.fq.gz,burbot-Mc12-P_S214_R2.cor.fq.gz,burbot-Md01-C_S185_R2.cor.fq.gz,burbot-Md02-C_S195_R2.cor.fq.gz,burbot-Md04-C_S205_R2.cor.fq.gz,burbot-Md05-C_S215_R2.cor.fq.gz,burbot-Md09-P_S186_R2.cor.fq.gz,burbot-Md10-P_S196_R2.cor.fq.gz,burbot-Md11-P_S206_R2.cor.fq.gz,burbot-Md12-P_S216_R2.cor.fq.gz,burbot-RV01-C_S187_R2.cor.fq.gz,burbot-RV02-C_S197_R2.cor.fq.gz,burbot-RV04-C_S207_R2.cor.fq.gz,burbot-RV06-C_S217_R2.cor.fq.gz,burbot-RV08-P_S188_R2.cor.fq.gz,burbot-RV09-P_S198_R2.cor.fq.gz,burbot-RV11-P_S208_R2.cor.fq.gz,burbot-RV12-P_S218_R2.cor.fq.gz"

echo $("Running trinity...")

/home/ubuntu/Trinityrnaseq-v2.6.6/Trinity --SS_lib_type RF --seqType fq --max_memory 180G --CPU 46 --output Rcorr_trinity_test --trimmomatic --left $R1 --right $R2

echo $(date)
echo "Done with Trinity! :)"

```

##Running script
```{bash}

chmod u+x run_Trinity.sh

nohup ./run_Trinity.sh > log_trinity_test.txt 2> log_trinity_error_test.txt &

```


Link to github

Publication

Copyright 


