---
title: "burbot_assembly_cleaned"
author: "Amanda"
date: "8/2/2018"
output: html_document
---

# List of references
http://oyster-river-protocol.readthedocs.io/en/latest/   
Note: Used ORP for help setting up AWS and installing necessary things, but the pipeline didn't actually work. I used Erin's workflow instead.

# Setting up an AWS instance for the first time

Followed the "Launch a Linux Virtual Machine" tutorial on AWS to learn how to create a new Amazon instance (https://aws.amazon.com/getting-started/tutorials/launch-a-virtual-machine/) 

Accoring to the Oyster River Protocol (https://github.com/macmanes-lab/orp_website/blob/master/aws_setup.md) we want to run everything on a standard Ubuntu 16.04 machine, so I selected this type of instance in AWS. Set up the instance with the default settings. Downloaded my key (mandykey), which is required to access the instance from the Terminal. 

You must move the key from the Downloads file to your ssh folder and make the key invisible: 

```{r Key to Access AWS, eval=F}
mv ~/Downloads/mandykey.pem ~/.ssh/mandykey.pem
chmod 400 ~/.ssh/mandykey.pem
```


## Connecting to AWS 

To connect to the instance in the Terminal, you have to go to the instance and retrieve the Public DNS (ec2... blah blah). This will change every time you log on, so you'll have to check every time. Now you can log on through the Terminal:

```{r Connecting to AWS, eval=F}

ssh -i ~/.ssh/mandykey.pem -l ubuntu ec2-35-166-55-218.us-west-2.compute.amazonaws.com

```

When you are done running whatever you are doing, go back to your AWS webpage, right click on "Running" under the "Instance State" tab, scroll to "Instance State" and click "Stop". Do not click "Terminate", because this will kill the instance. You need to stop your instance when you're done so that you don't get charged for hours that you're not actually using! 

## Create Alarms
What happens if you forget to stop your instance? What happens if a major job fails when you're using a higher powered (=expensive) computer and you don't realize? You get charged for hours that you're not using! This is why we want to create an alarm to stop the instance when it's not in use. Here are the settings I chose:
Under the "Monitoring" tab, select "Create Alarm": 
- Send a notification to "InstanceStopped" with these recipients: ajfrazier@ucdavis.edu
- Take the action "Stop this instance"
- Whenever "Average" of "CPU utilization" is <= 1% for at least 1 consecutive period(s) of 1 hour. 

Note: I ended up turning off the alarm because it drove me insane getting kicked off when I was running low-computation things. I was just careful to make sure to check my Instance often when I was running something to make sure I turned it off when I was done. It was also pretty easy to estimate how long something would take by monitoring the first few sequences it would run through. I simply used an alarm clock to remind myself to check my instance. 

## Install linuxy things from apt-get 
```{r Install Linuxy Things, eval=F}

sudo apt-get update && sudo apt-get -y upgrade && sudo apt-get -y install ruby build-essential mcl python python-pip default-jre

```

## Install linux brew
```{r Install Linux Brew, eval=F}

sudo mkdir /home/linuxbrew
sudo chown $USER:$USER /home/linuxbrew
git clone https://github.com/Linuxbrew/brew.git /home/linuxbrew/.linuxbrew
echo 'export PATH="/home/linuxbrew/.linuxbrew/bin:$PATH"' >> ~/.profile
echo 'export MANPATH="/home/linuxbrew/.linuxbrew/share/man:$MANPATH"' >> ~/.profile
echo 'export INFOPATH="/home/linuxbrew/.linuxbrew/share/info:$INFOPATH"' >> ~/.profile
source ~/.profile
brew tap brewsci/science
brew tap brewsci/bio
brew update
brew install gcc python metis parallel

```

## Install python modules
```{r Install Python Modules, eval=F}

pip install cvxopt numpy biopython scipy
pip install --upgrade pip

```

## Create a new volume on AWS
The only things that I want to keep on the root volume for my instance are the basic things (e.g. python, conda, etc.) and the programs I'm running (e.g. Trinity, Salmon, Trimmomatic, etc.). I do NOT want to store data there, mostly because the amount of storage space that I need will change a lot depending on where I am in the workflow. By storing data on separate volumes, I can more easily adjust the amount of storage that I'm paying for. All I have to do is connect the volume each time I start my instsance. 

On AWS webpage: 
1. Go to "Volumes" tab
2. Select "Create Volume" to create volume. Make sure the Availability Zone matches the root volume for that instance. 
3. Name the volume you just created.
Note: After you create the volume, it can take up to 6 hours until it's available for use. 
4. Right click on the Volume and select "Attach". Choose the AWS instance that you want to attach it to. Now, when you type "lsblk" in the Terminal, the Volume sould be available. 
In the Terminal:

```{r Create a New Volume, eval=F}
# 5. Format the volume. DO NOT RUN THIS AFTER THE FIRST TIME as it will reformat the volume and delete any data on it. 
sudo mkfs -t ext4 volumename   
sudo mkfs -t ext4 /dev/xvdg 

# 6. Make a directory where you want to mount your volume: 
sudo mkdir data

# 7. Mount the volume to that directory. This is where it will be in your file-tree: 
sudo mount /dev/xvdg data

# 8. Give yourself ownership of the new volume: 
sudo chown ubuntu -R data 

# 9. Move into the new volume: 
cd data 

# 10. Check what you have available to you:
df -h

```

# Download data from the genome center 

## Download to blue hard drive for backup 

1. Make a directory on hard drive and move into that directory: 

```{r make directory, eval=F}
mkdir ~/Mandy/Burbot_DE/

cd ~/Mandy/Burbot_DE/
```

2. Download per lane using rsync (as per genome center instructions)
The '&' has that command run in the background. This way you can download all of your data files at once.
The random string at the end identifies which directory will be downloaded. You get this from the SLIMS account.

```{r copy data, eval=F}
rsync -avL slimsdata.genomecenter.ucdavis.edu::slims/viy05v2229 . &
rsync -avL slimsdata.genomecenter.ucdavis.edu::slims/xzsdtmj958 . &
rsync -avL slimsdata.genomecenter.ucdavis.edu::slims/8tq5hvxqm . &
rsync -avL slimsdata.genomecenter.ucdavis.edu::slims/w4h63my7gq . &
```


# FastQC and MultiQC
I just added this script to each of the four folders and changed the folder name each time. This gave me FastQC files for each sequence for each lane, so if there were any lane issues it would be obvious.
## FastQC
```{r fastq.sh, eval=F}

#!/bin/bash

for f in *.gz
	do
		/Users/amanda/Applications/FastQC/fastqc $f -o /Volumes/Burbot/Burbot_DE/8tq5hvxqm/Unaligned/Project_ATMF_L4_H1181P_Frazier/fastqc_output
	done

```

## MultiQC
Again, I did this for each of the four lanes. 
```{r multiqc, eval=F}

cd /Volumes/Burbot/Burbot_DE/8tq5hvxqm/Unaligned/Project_ATMF_L4_H1181P_Frazier/fastqc_output 

multiqc . 

```


# Download sequence data to AWS 
Download sequence data to AWS using a script called getmydata.sh. Since I have four data directories on SLIMS, I just changed the random string at the end for the four directories and ran the script 4 times. Could do it in parallel, but not sure how...
```{r gemydata.sh, eval=F}

#!/bin/bash

rsync -avL slimsdata.genomecenter.ucdavis.edu::slims/w4h63my7gq .

```

Run the script on background and record the log:
```{r run getmydata.sh, eval=F}

chmod u+x ./getmydata.sh

nohup ./getmydata.sh >datatransferlog.txt &

```

# Check for data corruption.
## Check number of files 
Count number of files in directory to check if all the files are there: 
```{r count number of files in directory, eval=F}
ls -l | wc -l 
```

## Check md5
Note that on a mac, the command to check md5 is just md5. In linux, it's md5sum. The output is also slightly different, but it gives you the same md5 hash. If you run md5sum in linux (on AWS) you'll get the output that matches the md5.txt that the genome center gave you. This way you can just compare the two files using diff. If the files match, diff won't print anything. 
```{r md5, eval=F}

diff <(md5sum *.fastq.gz | sort -u) <(cat md5.txt | sort -u)

# All files matched, meaning that data transferred without corruption! 

```

# Concatenate files
Script to concatenate the four files from each lane together called "catmyR1.sh". This way I'll only have one R1 file for each fish instead of four. 
```{r catmyR1.sh, eval=F}

#!/bin/bash

mkdir concat
for file_start in burbot-Ma01*R1* burbot-Ma02*R1* burbot-Ma03*R1* burbot-Ma04*R1* burbot-Ma07*R1* burbot-Ma10*R1* burbot-Ma11*R1* burbot-Ma12*R1* burbot-Mb01*R1* burbot-Mb02*R1* burbot-Mb03*R1* burbot-Mb05*R1* burbot-Mb08*R1* burbot-Mb09*R1* burbot-Mb10*R1* burbot-Mb12*R1* burbot-Mc01*R1* burbot-Mc02*R1* burbot-Mc03*R1* burbot-Mc04*R1* burbot-Mc08*R1* burbot-Mc10*R1* burbot-Mc11*R1* burbot-Mc12*R1* burbot-Md01*R1* burbot-Md02*R1* burbot-Md04*R1* burbot-Md05*R1* burbot-Md09*R1* burbot-Md10*R1* burbot-Md11*R1* burbot-Md12*R1* burbot-RV01*R1* burbot-RV02*R1* burbot-RV04*R1* burbot-RV06*R1* burbot-RV08*R1* burbot-RV09*R1* burbot-RV11*R1* burbot-RV12*R1*
do  
  #List the file names we want to concatenate 
  allR1=`ls ./*/Unaligned*/*/${file_start}.fastq.gz`
  #Make the basename of the new concatenated file
  anR1=`ls ./xzsdtmj958/Unaligned*/*/${file_start}.fastq.gz`
  outfile=$(basename $anR1 _L002_R1_001.fastq.gz)
  #Check the new file basename
  echo ${outfile}_R1 
  #Concatenate and write to a new file
  cat $allR1 > ./concat/${outfile}_R1.fastq.gz
done

```

Running catmyR1.sh:
```{r run catmyR1.sh, eval=F}

chmod u+x catmyR1.sh

nohup ./catmyR1.sh >catmyR1Log.txt &

```

And again for R2 called "catmyR2.sh"
```{r catmyR2.sh, eval=F}

#!/bin/bash

for file_start in burbot-Ma01*R2* burbot-Ma02*R2* burbot-Ma03*R2* burbot-Ma04*R2* burbot-Ma07*R2* burbot-Ma10*R2* burbot-Ma11*R2* burbot-Ma12*R2* burbot-Mb01*R2* burbot-Mb02*R2* burbot-Mb03*R2* burbot-Mb05*R2* burbot-Mb08*R2* burbot-Mb09*R2* burbot-Mb10*R2* burbot-Mb12*R2* burbot-Mc01*R2* burbot-Mc02*R2* burbot-Mc03*R2* burbot-Mc04*R2* burbot-Mc08*R2* burbot-Mc10*R2* burbot-Mc11*R2* burbot-Mc12*R2* burbot-Md01*R2* burbot-Md02*R2* burbot-Md04*R2* burbot-Md05*R2* burbot-Md09*R2* burbot-Md10*R2* burbot-Md11*R2* burbot-Md12*R2* burbot-RV01*R2* burbot-RV02*R2* burbot-RV04*R2* burbot-RV06*R2* burbot-RV08*R2* burbot-RV09*R2* burbot-RV11*R2* burbot-RV12*R2*
do  
  #List the file names we want to concatenate 
  allR2=`ls ./*/Unaligned*/*/${file_start}.fastq.gz`
  #Make the basename of the new concatenated file
  anR2=`ls ./xzsdtmj958/Unaligned*/*/${file_start}.fastq.gz`
  outfile=$(basename $anR2 _L002_R2_001.fastq.gz)
  #Check the new file basename
  echo ${outfile}_R2 
  #Concatenate and write to a new file
  cat $allR2 > ./concat/${outfile}_R2.fastq.gz
done

```


Running catmyR2.sh:
```{r run catmyR2.sh, eval=F}

chmod u+x catmyR2.sh

nohup ./catmyR2.sh >catmyR2Log.txt &

```

# Rcorrector

```{r runRcorrector.sh, eval=F}

#!/bin/bash 

for infile in *R1.fastq.gz
do
    base=$(basename $infile R1.fastq.gz)
    echo "Running ${base}"
    perl ~/rcorrector/run_rcorrector.pl -1 ${base}R1.fastq.gz -2 ${base}R2.fastq.gz -k 32 -t 46 -od ../Rcorrected
    echo "Done with ${base}... moving on"
done
```
For rcorrector, specify a kmer length of 32 (this is the max kmer length). This makes sense for your data because you have long reads. Default is 23. 
t=46 is 46 threads to parallelize to decrease running time. 

Erin's took 20 hours to run. Mine took ~40 hours to run on m5.12xlarge instance

# Trinity 

```{r Trinity Script, eval=FALSE}

#!/bin/bash

echo $(date)

cd /home/ubuntu/data/2_Rcorrector/Rcorrected2/

R1="burbot-Ma01-C_S179_R1.cor.fq.gz,burbot-Ma02-C_S189_R1.cor.fq.gz,burbot-Ma03-C_S199_R1.cor.fq.gz,burbot-Ma04-C_S209_R1.cor.fq.gz,burbot-Ma07-P_S180_R1.cor.fq.gz,burbot-Ma10-P_S190_R1.cor.fq.gz,burbot-Ma11-P_S200_R1.cor.fq.gz,burbot-Ma12-P_S210_R1.cor.fq.gz,burbot-Mb01-C_S181_R1.cor.fq.gz,burbot-Mb02-C_S191_R1.cor.fq.gz,burbot-Mb03-C_S201_R1.cor.fq.gz,burbot-Mb05-C_S211_R1.cor.fq.gz,burbot-Mb08-P_S182_R1.cor.fq.gz,burbot-Mb09-P_S192_R1.cor.fq.gz,burbot-Mb10-P_S202_R1.cor.fq.gz,burbot-Mb12-P_S212_R1.cor.fq.gz,burbot-Mc01-C_S183_R1.cor.fq.gz,burbot-Mc02-C_S193_R1.cor.fq.gz,burbot-Mc03-C_S203_R1.cor.fq.gz,burbot-Mc04-C_S213_R1.cor.fq.gz,burbot-Mc08-P_S184_R1.cor.fq.gz,burbot-Mc10-P_S194_R1.cor.fq.gz,burbot-Mc11-P_S204_R1.cor.fq.gz,burbot-Mc12-P_S214_R1.cor.fq.gz,burbot-Md01-C_S185_R1.cor.fq.gz,burbot-Md02-C_S195_R1.cor.fq.gz,burbot-Md04-C_S205_R1.cor.fq.gz,burbot-Md05-C_S215_R1.cor.fq.gz,burbot-Md09-P_S186_R1.cor.fq.gz,burbot-Md10-P_S196_R1.cor.fq.gz,burbot-Md11-P_S206_R1.cor.fq.gz,burbot-Md12-P_S216_R1.cor.fq.gz,burbot-RV01-C_S187_R1.cor.fq.gz,burbot-RV02-C_S197_R1.cor.fq.gz,burbot-RV04-C_S207_R1.cor.fq.gz,burbot-RV06-C_S217_R1.cor.fq.gz,burbot-RV08-P_S188_R1.cor.fq.gz,burbot-RV09-P_S198_R1.cor.fq.gz,burbot-RV11-P_S208_R1.cor.fq.gz,burbot-RV12-P_S218_R1.cor.fq.gz"

R2="burbot-Ma01-C_S179_R2.cor.fq.gz,burbot-Ma02-C_S189_R2.cor.fq.gz,burbot-Ma03-C_S199_R2.cor.fq.gz,burbot-Ma04-C_S209_R2.cor.fq.gz,burbot-Ma07-P_S180_R2.cor.fq.gz,burbot-Ma10-P_S190_R2.cor.fq.gz,burbot-Ma11-P_S200_R2.cor.fq.gz,burbot-Ma12-P_S210_R2.cor.fq.gz,burbot-Mb01-C_S181_R2.cor.fq.gz,burbot-Mb02-C_S191_R2.cor.fq.gz,burbot-Mb03-C_S201_R2.cor.fq.gz,burbot-Mb05-C_S211_R2.cor.fq.gz,burbot-Mb08-P_S182_R2.cor.fq.gz,burbot-Mb09-P_S192_R2.cor.fq.gz,burbot-Mb10-P_S202_R2.cor.fq.gz,burbot-Mb12-P_S212_R2.cor.fq.gz,burbot-Mc01-C_S183_R2.cor.fq.gz,burbot-Mc02-C_S193_R2.cor.fq.gz,burbot-Mc03-C_S203_R2.cor.fq.gz,burbot-Mc04-C_S213_R2.cor.fq.gz,burbot-Mc08-P_S184_R2.cor.fq.gz,burbot-Mc10-P_S194_R2.cor.fq.gz,burbot-Mc11-P_S204_R2.cor.fq.gz,burbot-Mc12-P_S214_R2.cor.fq.gz,burbot-Md01-C_S185_R2.cor.fq.gz,burbot-Md02-C_S195_R2.cor.fq.gz,burbot-Md04-C_S205_R2.cor.fq.gz,burbot-Md05-C_S215_R2.cor.fq.gz,burbot-Md09-P_S186_R2.cor.fq.gz,burbot-Md10-P_S196_R2.cor.fq.gz,burbot-Md11-P_S206_R2.cor.fq.gz,burbot-Md12-P_S216_R2.cor.fq.gz,burbot-RV01-C_S187_R2.cor.fq.gz,burbot-RV02-C_S197_R2.cor.fq.gz,burbot-RV04-C_S207_R2.cor.fq.gz,burbot-RV06-C_S217_R2.cor.fq.gz,burbot-RV08-P_S188_R2.cor.fq.gz,burbot-RV09-P_S198_R2.cor.fq.gz,burbot-RV11-P_S208_R2.cor.fq.gz,burbot-RV12-P_S218_R2.cor.fq.gz"

echo $("Running trinity...")

/home/ubuntu/Trinityrnaseq-v2.6.6/Trinity --SS_lib_type RF --seqType fq --max_memory 180G --CPU 46 --output Rcorr_trinity_test --trimmomatic --left $R1 --right $R2

echo $(date)
echo "Done with Trinity! :)"

```

Note that I am sticking with the default of running trimmomatic within Trinity. Some people run Trimmomatic sepaarately so you can specifcy the parameters, but I think it's fine to use the defaults (listed below) because the sequence data is very high quality. This means that relatively gentle trimming (which is the default) should be totally fine. 

```{r trimmomatic parameters, eval=F}

Trinity --show_full_usage_info

#  --quality_trimming_params <string>   defaults to: "ILLUMINACLIP:/home/ubuntu/Trinityrnaseq-v2.6.6/trinity-plugins/Trimmomatic/adapters/TruSeq3-PE.fa:2:30:10 SLIDINGWINDOW:4:5 LEADING:5 TRAILING:5 MINLEN:25"
```

Also note that for Trinity I am sticking with the default of normalization. This is recommended for very large data sets. For example, Erin's crashed when she tried to run without normalization. Also, based on my small test data set, the noramlization really doesn't change anything that much. 


```{r run_Trinity.sh, eval=F}

chmod u+x run_Trinity.sh

nohup ./run_Trinity.sh > log_trinity.txt 2> log_trinity_error.txt &

```

Trinity Normalization Stats that are printed when it runs: 
```{r Trinity Normalization Stats, eval=F}

#CMD: /home/ubuntu/Trinityrnaseq-v2.6.6/util/..//util/support_scripts//nbkc_normalize.pl --stats_file pairs.K25.stats --max_cov 50  --min_cov 1 --max_pct_stdev 10000 > pairs.K25.stats.C50.pctSD10000.accs
#40962711 / 1204122769 = 3.40% reads selected during normalization.
#0 / 1204122769 = 0.00% reads discarded as likely aberrant based on coverage profiles.
#0 / 1204122769 = 0.00% reads missing kmer coverage (N chars included?).
#0 / 1204122769 = 0.00% reads discarded as below minimum coverage threshold=1

```


# Increase AWS ROOT Volume Size
I ran out of space on the root volume when trying to run Trinity (I think this is because Java and the programs are installed on the root, so any temporary files might be there instead of on the detachable volume where I have my data?) 
1. Go to AWS online console and select the instance. In the description tab you can see the root volume. Check what the volume ID is. 
2. Go to the Volumes and select the same volume ID for the root volume. 
3. Right click and select "Modify Volume"
4. Type in the desired memory. 
5. Go to Instances and reboot the instance. 
6. Log back into the instance in the Terminal and type "lsblk". The root volume should now have the space you just added. 

# Increase AWS EBS volume size
After running out of space on the root volume I also ran out of space on the EBS volume for Trinity. 
1. Go to the "Volumes" tab and select the volume you want to expand and click "Modify Volume". Enter the size you want (I increased from 1TB to 2TB). 
2. In the terminal: 
```{bash}
lsblk #List volumes
du -h #List data usage, human-readable. This doesn't show what's available, only what's used. 
df -h #List free data space. Note that this can be different than what you see using lsblk! This is because the file system needs to be extended before you can use the available space. 

sudo resize2fs /dev/nvme1n1

```

Note that increasing to 2TB STILL wasn't enough space. The script got through trimmomatic, but in normalization it crashed due to lack of space. 
I doubled the volume to 4TB. 
Tried rerunning without deleting anything in hopes that it would pick up where it left off, but it couldn't figure it out. So I deleted the in_silico_normalization file (but NOT the trimmomatic files) so it could pick up there. Reran (again) and it finally worked! Only took ~1.5 days to run!

# Trinity Stats
```{r check Trinity Stats, eval=F}
cd ~/Trinityrnaseq-v2.6.6/util
./TrinityStats.pl ~/data/2_Rcorrector/Rcorrected2/Rcorr_trinity/Trinity.fasta

```

Output: 

################################
 Counts of transcripts, etc.
################################
Total trinity 'genes':	435317
Total trinity transcripts:	793202
Percent GC: 47.58

########################################
Stats based on ALL transcript contigs:
########################################

	Contig N10: 5625
	Contig N20: 4100
	Contig N30: 3185
	Contig N40: 2507
	Contig N50: 1959

	Median contig length: 592
	Average contig: 1097.34
	Total assembled bases: 870415829


#####################################################
 Stats based on ONLY LONGEST ISOFORM per 'GENE':
#####################################################

	Contig N10: 4754
	Contig N20: 3220
	Contig N30: 2267
	Contig N40: 1611
	Contig N50: 1159

	Median contig length: 434
	Average contig: 765.83
	Total assembled bases: 333377568
###

# Reducing Volume size 
After running Trinity, I don't need to have such huge volumes (and I really don't want to spend a crap ton of money for storage space that I don't need!)

You can't make your volume smaller, so you have to make a new smaller volume, mount both volumes to the AWS instance in different directories, then copy the data to the smaller volume and then delete the origianl volume.

# Assessing quality of transcriptome assembly 
## BUSCO 
BUSCO (Benchmarking Universal Single-Copy Orthologs) is used to assess how complete the transcriptome is based on what is expected from evolutionarily conserved sequences. If the highly conserved sequences aren't found in my transcriptome, that indicates that the assembly didn't include everything that was expected (due to not-great sequencing, assembly, or alignment).


```{r install BUSCO, eval=F}
conda install busco 
```

```{r download fish dataset, eval=F}
wget https://busco.ezlab.org/datasets/actinopterygii_odb9.tar.gz
gunzip actinopterygii_odb9.tar.gz
tar -xvf actinopterygii_odb9.tar
```

```{r BUSCO on actinopterygii, eval=F}

run_BUSCO.py -i Trinity.fasta -o burbot_busco_fish \
-l actinopterygii_odb9 -m transcriptome --cpu 4

nohup ./run_BUSCO.sh > log_BUSCO.txt 2> log_BUSCO_error.txt &

```

Burbot transcriptome against actinopterygii BUSCO results: 
C:81.0%[S:27.9%,D:53.1%],F:13.6%,M:5.4%,n:4584
Running time: ~3 days 


```{r BUSCO on eukaryota, eval=F}
#Download eukaryota dataset
wget https://busco.ezlab.org/datasets/eukaryota_odb9.tar.gz
gunzip eukaryota_odb9.tar.gz
tar -xvf eukaryota_odb9.tar

#Script
run_BUSCO.py -i Trinity.fasta -o burbot_busco_eukaryota \
-l eukaryota_odb9 -m transcriptome --cpu 4

#Run script on background 
nohup ./run_BUSCO.sh > log_BUSCO_eukaryota.txt 2> log_BUSCO_error_eukaryota.txt & 

```

Burbot transcriptome against eukaryota BUSCO results: 
C:97.4%[S:24.8%,D:72.6%],F:2.6%,M:0.0%,n:303 
Running time: <1 hr 


```{r BUSCO on metazoa, eval=F}
#Download metazoa dataset 
wget https://busco.ezlab.org/datasets/metazoa_odb9.tar.gz 
gunzip metazoa_odb9.tar.gz
tar -xvf metazoa_odb9.tar

#Script
run_BUSCO.py -i Trinity.fasta -o burbot_busco_metazoa \
-l metazoa_odb9 -m transcriptome --cpu 4

#Run script on background
nohup ./run_BUSCO.sh > log_BUSCO_metazoa.txt 2> log_BUSCO_error_metazoa.txt & 
```

Burbot transcriptome against metazoa BUSCO results: 
C:97.4%[S:28.4%,D:69.0%],F:2.6%,M:-0.0%,n:978
Running time: ~2.5 hrs 


To run BUSCO on genomes, there are a few more requirements. Must install Augustus and add to path. 
```{r set up BUSCO for genomes}
cd 
wget http://bioinf.uni-greifswald.de/augustus/binaries/augustus-3.3.1.tar.gz

export AUGUSTUS_CONFIG_PATH="/home/ubuntu/augustus-3.3.1/config/"    #Note that for some reason this line has to be run every time before you start running a genome BUSCO... not sure why but it works if you do that. 

```

```{r BUSCO Atlantic cod against actinopterygii, eval=F}
#Download Atlantic cod genome 
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/231/765/GCA_000231765.1_GadMor_May2010/GCA_000231765.1_GadMor_May2010_genomic.fna.gz
gunzip GCA_000231765.1_GadMor_May2010_genomic.fna.gz

#Script 
run_BUSCO.py -i GCA_000231765.1_GadMor_May2010_genomic.fna -o cod_busco_actinopterygii \
-l actinopterygii_odb9 -m genome --cpu 4

#Run script on background 
nohup ./run_BUSCO.sh > log_BUSCO_cod.txt 2> log_BUSCO_error_cod.txt & 
  
```

Atlantic cod genome against actinopterygii BUSCO results: 
C:58.9%[S:57.6%,D:1.3%],F:17.9%,M:23.2%,n:4584
Running time: 


```{r BUSCO polar cod against atinopterygii, eval=F}
#Download polar cod genome 
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/900/302/515/GCA_900302515.1_ASM90030251v1/GCA_900302515.1_ASM90030251v1_genomic.fna.gz
gunzip GCA_900302515.1_ASM90030251v1_genomic.fna.gz

#Script 
run_BUSCO.py -i GCA_900302515.1_ASM90030251v1_genomic.fna -o polarcod_busco_actinopterygii \
-l actinopterygii_odb9 -m genome --cpu 4

#Run script on background 
nohup ./run_BUSCO.sh > log_BUSCO_polarcod.txt 2> log_BUSCO_error_polarcod.txt & 

```

Polar cod genome against actinopterygii BUSCO results: 
C:34.0%[S:33.4%,D:0.6%],F:25.7%,M:40.3%,n:4584
Running time: ~13 hrs

```{r BUSCO published Lota lota genome against actinopterygii, eval=F}
#Download published Lota lota genome 
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/900/302/385/GCA_900302385.1_ASM90030238v1/GCA_900302385.1_ASM90030238v1_genomic.fna.gz
gunzip GCA_900302385.1_ASM90030238v1_genomic.fna.gz

#Script
run_BUSCO.py -i GCA_900302385.1_ASM90030238v1_genomic.fna -o lotapubgenome_busco_actinopterygii \
-l actinopterygii_odb9 -m genome --cpu 4

#Run script on background
nohup ./run_BUSCO.sh > log_BUSCO_lotapubgenome.txt 2> log_BUSCO_error_lotapubgenome.txt &
  
```

Lota lota genome against actinopterygii BUSCO results:
C:48.3%[S:47.5%,D:0.8%],F:23.4%,M:28.3%,n:4584
Running time: ~12 hrs 


Note: It turns out that there IS a polar cod transcriptome!! And check to see if there is a burbot and Atlantic cod transcriptome. Make sure that when you return from Antarctica you run BUSCO comparing existing transcriptomes against fish dataset to compare transcriptome qualities!!!! 


# Hibernating AWS Instance

I have a ton of data that is just sitting on AWS that we are being charged for. So, I am going to download all of this to my external hard drive (and the lab computer, and my personal hard drive at home) to store it for the next few months. I'll leave the programs on AWS, though, so if I need to I can return to this instance relatively easily. 

1. Cleaned up folders to get rid of unnecessary files and duplicate files. Of the RCorrected files, I am only saving the Rcorrected, paired, trimmed files. I can use these for mapping later. I kept log files of everything, but am deleting intermediate files (e.g. all the intermediate files from Trinity). 
2. Copy mandykey.pem to lab mac (I put it in the Mandy folder), that way if it takes a while to download I'm not stuck leaving my laptop here. I am copying using the lab mac, but copying the data to my Burbot external harddrive because there isn't enough space on the lab mac. You have to make the key usable after copying: 
```{R chmod key}
cd ~/Mandy
chmod 600 mandykey.pem 
```
3. Run a test copy: 
```{r test copy}
scp -i ~/Mandy/mandykey.pem -r ubuntu@ec2-34-222-149-64.us-west-2.compute.amazonaws.com:/home/ubuntu/data/test /Volumes/Burbot/Burbot_DE/AWSCopy &  
```
4. Copy the entire data directory from AWS: 
```{r copy AWS}
scp -i ~/Mandy/mandykey.pem -r ubuntu@ec2-34-222-149-64.us-west-2.compute.amazonaws.com:/home/ubuntu/data/ /Volumes/Burbot/Burbot_DE/AWSCopy & 
```
5. Check the status of the copy. I am expecting 210 GB total. 
```{r Check Status}
cd /Volumes/Burbot/Burbot_DE/AWSCopy/

du -h
```
It took 6 hours to download everything. 
6. Check for data corruption: 
- Total size of data directory matches at 208GB. That's a good indicator that everything transferred successfully. All folders are there. 
- Ran md5 on burbot_trinity.fasta, it matched pre & post transfer. This is the most important thing to transfer successfully as I don't want to re-run Trinity.
- Ran md5 on all the Rcorrected, paired, trimmed sequences for each fish. I visually compared the last few digits in each and they all matched. Seems good to me. 
7. Deleted entire data volume (2TB) on AWS. Left root volume w/ programs (50 GB). 







